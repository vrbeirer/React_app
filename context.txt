

# ğŸ”µ Q1: What is the fundamental intuition behind a linear model?

A linear model assumes the output changes in a straight-line way when inputs change.
Like â€œmore square feet â†’ more house price.â€ 

----------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”µ Q2. How is this linear relationship represented mathematically? Explain the formula and its components.

`h(X) = Î¸0 + Î¸1X1`
Î¸0 is starting value.
Î¸1 tells how much output changes when input changes. 

 This is the familiar equation of a line (y = c + mx).
 : The predicted output value. [ also; "the Hypothesis" (rings a bell?)]
 : The input feature value (e.g., square footage).
 (theta_1): This is the weight or coefficient. It corresponds to the slope (m) and defines how much the output h(X) changes
 for a one-unit change in .
 (theta_0): This is the bias or intercept. It corresponds to the y-intercept (c) and is the baseline prediction when all input
 features are zero

-----------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”µ Q3. How do we implement this generalized linear formula in core Python?

Add bias + sum of (weight Ã— feature).
This is done in `_compute_z()` in the PDF. 

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”µ Q4.  What is the expected structure of the input data (X) and target data (y) for a model?

* X is a list of samples. Each sample is a list of features.
* y is a list of labels (one per sample). 

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”µ Q5. Why linear output is not good for classification?

Because it can be any number, like 300 or -50.
A class needs a probability between 0 and 1. 

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”µ Q6.  What is the primary modification needed to convert a linear regression model into a logistic
 regression model?

Wrap the linear output z inside a Sigmoid function. 

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”µ Q7.  What is the formula for the Sigmoid function, and how does it behave?

`g(z) = 1 / (1 + e^-z)`
Big positive z â†’ output near 1.
Big negative z â†’ output near 0.
z=0 â†’ output = 0.5. 

----------------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”µ Q8. Show the Python implementation of the Sigmoid function and the new hypothesis.

Same as in the PDFâ€™s `_sigmoid` function, with checks for big numbers. 

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”µ Q9. Why sigmoid output is a valid probability?

Because it comes from **odds** and **log-odds**, which map naturally into [0,1].
The math links linear z â†’ probability p. 

------------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”µ Q10. Probability, Odds, Log-Odds

* Probability: p (0 to 1)
* Odds: p/(1-p) (0 to âˆ)
* Log-Odds: log(p/(1-p)) (-âˆ to âˆ) 

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”µ Q11. Derive sigmoid (simple explanation)

Set log-odds equal to linear z.
Solve the equation for p.
You get the sigmoid formula. 

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”µ Q12. How are all these components assembled into a trainable model? Explain the role of the Cost
 Function and Gradient Descent.

Two things:

1. **Cost Function:** Measures how wrong prediction is (log-loss).
2. **Gradient Descent:** Adjusts weights step-by-step to lower the cost. 

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”µ Q13. Full class code

Exactly as shown in the PDF.
It contains: sigmoid, cost, gradients, fit, predict. 

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”µ Q14. : How do we test this model? Provide the setup for a sample experiment

Use a simple dataset (hours studied).
Train the model.
Predict on new samples. 

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”µ Q15.  What is the expected output of the experiment, and how should it be interpreted?

* Cost goes down over iterations.
* Final theta values learned.
* Predictions:

  * Small hours â†’ fail
  * Large hours â†’ pass
  * Middle ~3 hours â†’ around 0.5 probability. 

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”µ Q16. Main limitation of this pure Python version

It is slow because of loops.
Libraries like NumPy are much faster. 

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

# ğŸ”µ Q17. What happens if you change learning rate or iterations?

* Very high learning rate â†’ model explodes.
* Very low â†’ learning is very slow.
* Very few iterations â†’ underfitting. 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
